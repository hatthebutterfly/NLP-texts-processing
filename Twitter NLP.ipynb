{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbfa0112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"train_data.drop(columns=['id','keyword','location'],inplace=True)\\ntest_data.drop(columns=['keyword','location'],inplace=True)\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "import emoji\n",
    "import keras\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from cleantext import clean\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import (feature_extraction, linear_model, model_selection, preprocessing)\n",
    "import textblob\n",
    "from textblob import Word\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "train_data=pd.read_csv(r'C:\\downloded dataset\\twitter disater\\processed_data\\spell_corrected_text_tt_final.csv')\n",
    "test_data=pd.read_csv(r'C:\\downloded dataset\\twitter disater\\processed_data\\test_spell_corrected_text_tt_final.csv')\n",
    "print(len(train_data))\n",
    "'''train_data.drop(columns=['id','keyword','location'],inplace=True)\n",
    "test_data.drop(columns=['keyword','location'],inplace=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acce9676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AFAIK': 'As Far As I Know', 'AFK': 'Away From Keyboard', 'ASAP': 'As Soon As Possible', 'ATK': 'At The Keyboard', 'ATM': 'At The Moment', 'A3': 'Anytime, Anywhere, Anyplace', 'BAK': 'Back At Keyboard', 'BBL': 'Be Back Later', 'BBS': 'Be Back Soon', 'BFN': 'Bye For Now', 'B4N': 'Bye For Now', 'BRB': 'Be Right Back', 'BRT': 'Be Right There', 'BTW': 'By The Way', 'B4': 'Before', 'CU': 'See You', 'CUL8R': 'See You Later', 'CYA': 'See You', 'FAQ': 'Frequently Asked Questions', 'FC': 'Fingers Crossed', 'FWIW': \"For What It's Worth\", 'FYI': 'For Your Information', 'GAL': 'Get A Life', 'GG': 'Good Game', 'GN': 'Good Night', 'GMTA': 'Great Minds Think Alike', 'GR8': 'Great!', 'G9': 'Genius', 'IC': 'I See', 'ICQ': 'I Seek you (also a chat program)', 'ILU': 'ILU: I Love You', 'IMHO': 'In My Honest/Humble Opinion', 'IMO': 'In My Opinion', 'IOW': 'In Other Words', 'IRL': 'In Real Life', 'KISS': 'Keep It Simple, Stupid', 'LDR': 'Long Distance Relationship', 'LMAO': 'Laugh My A.. Off', 'LOL': 'Laughing Out Loud', 'LTNS': 'Long Time No See', 'L8R': 'Later', 'MTE': 'My Thoughts Exactly', 'M8': 'Mate', 'NRN': 'No Reply Necessary', 'OIC': 'Oh I See', 'PITA': 'Pain In The A..', 'PRT': 'Party', 'PRW': 'Parents Are Watching', 'ROFL': 'Rolling On The Floor Laughing', 'ROFLOL': 'Rolling On The Floor Laughing Out Loud', 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off', 'SK8': 'Skate', 'STATS': 'Your sex and age', 'ASL': 'Age, Sex, Location', 'THX': 'Thank You', 'TTFN': 'Ta-Ta For Now!', 'TTYL': 'Talk To You Later', 'U': 'You', 'U2': 'You Too', 'U4E': 'Yours For Ever', 'WB': 'Welcome Back', 'WTF': 'What The F...', 'WTG': 'Way To Go!', 'WUF': 'Where Are You From?', 'W8': 'Wait...', '7K': 'Sick:-D Laugher'}\n",
      "Guys! Be Right Back need to go. I will be Away From Keyboard next couple of mins\n"
     ]
    }
   ],
   "source": [
    "#將縮寫還原\n",
    "CHAT_WORDS_STR = \"\"\"\n",
    "AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher\n",
    "\"\"\"\n",
    "chat_words_dict={}\n",
    "chat_words_list=set()\n",
    "for line in CHAT_WORDS_STR.split('\\n'):\n",
    "    if line!='':\n",
    "        shortcut=line.split('=')[0]\n",
    "        ori_words=line.split('=')[1]\n",
    "        chat_words_list.add(shortcut)\n",
    "        chat_words_dict[shortcut]=ori_words #建立縮寫字的字典\n",
    "print(chat_words_dict)\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    text=str(text)\n",
    "    for word in text.split():\n",
    "        if word.upper() in chat_words_dict.keys(): \n",
    "            text=text.replace(word,chat_words_dict[word.upper()])\n",
    "    return text\n",
    "#context = 'Guys! brb need to go. I will be AFK next couple of mins'\n",
    "#print(chat_words_conversion(context))\n",
    "train_data['text']=train_data['text'].apply(lambda text:chat_words_conversion(text))\n",
    "test_data['text']=test_data['text'].apply(lambda text:chat_words_conversion(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6dfcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#處理縮寫字(contraction)，例如：I'm \n",
    "\n",
    "def convert_contraction(text):\n",
    "    correct_word=[]\n",
    "    for word in text.split():\n",
    "        if word not in contractions.contractions_dict:\n",
    "            correct_word.append(word)\n",
    "        elif word in contractions.contractions_dict:\n",
    "            correct_word.append(contractions.fix(word))\n",
    "    return ' '.join(correct_word)\n",
    "example = \"I've worked so hard today. I'm going to run to home!\"\n",
    "print(convert_contraction(example))\n",
    "#train_data['text']=train_data['text'].apply(lambda text:convert_contraction(text))\n",
    "#test_data['text']=test_data['text'].apply(lambda text:convert_contraction(text))#remove stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea659c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#處理emoticons (-_-)\n",
    "EMOTICONS = {\n",
    "    u\":‑\\)\":\"Happy face or smiley\",\n",
    "    u\":\\)\":\"Happy face or smiley\",\n",
    "    u\":-\\]\":\"Happy face or smiley\",\n",
    "    u\":\\]\":\"Happy face or smiley\",\n",
    "    u\":-3\":\"Happy face smiley\",\n",
    "    u\":3\":\"Happy face smiley\",\n",
    "    u\":->\":\"Happy face smiley\",\n",
    "    u\":>\":\"Happy face smiley\",\n",
    "    u\"8-\\)\":\"Happy face smiley\",\n",
    "    u\":o\\)\":\"Happy face smiley\",\n",
    "    u\":-\\}\":\"Happy face smiley\",\n",
    "    u\":\\}\":\"Happy face smiley\",\n",
    "    u\":-\\)\":\"Happy face smiley\",\n",
    "    u\":c\\)\":\"Happy face smiley\",\n",
    "    u\":\\^\\)\":\"Happy face smiley\",\n",
    "    u\"=\\]\":\"Happy face smiley\",\n",
    "    u\"=\\)\":\"Happy face smiley\",\n",
    "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":-\\)\\)\":\"Very happy\",\n",
    "    u\":‑\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
    "    u\":@\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":'‑\\(\":\"Crying\",\n",
    "    u\":'\\(\":\"Crying\",\n",
    "    u\":'‑\\)\":\"Tears of happiness\",\n",
    "    u\":'\\)\":\"Tears of happiness\",\n",
    "    u\"D‑':\":\"Horror\",\n",
    "    u\"D:<\":\"Disgust\",\n",
    "    u\"D:\":\"Sadness\",\n",
    "    u\"D8\":\"Great dismay\",\n",
    "    u\"D;\":\"Great dismay\",\n",
    "    u\"D=\":\"Great dismay\",\n",
    "    u\"DX\":\"Great dismay\",\n",
    "    u\":‑O\":\"Surprise\",\n",
    "    u\":O\":\"Surprise\",\n",
    "    u\":‑o\":\"Surprise\",\n",
    "    u\":o\":\"Surprise\",\n",
    "    u\":-0\":\"Shock\",\n",
    "    u\"8‑0\":\"Yawn\",\n",
    "    u\">:O\":\"Yawn\",\n",
    "    u\":-\\*\":\"Kiss\",\n",
    "    u\":\\*\":\"Kiss\",\n",
    "    u\":X\":\"Kiss\",\n",
    "    u\";‑\\)\":\"Wink or smirk\",\n",
    "    u\";\\)\":\"Wink or smirk\",\n",
    "    u\"\\*-\\)\":\"Wink or smirk\",\n",
    "    u\"\\*\\)\":\"Wink or smirk\",\n",
    "    u\";‑\\]\":\"Wink or smirk\",\n",
    "    u\";\\]\":\"Wink or smirk\",\n",
    "    u\";\\^\\)\":\"Wink or smirk\",\n",
    "    u\":‑,\":\"Wink or smirk\",\n",
    "    u\";D\":\"Wink or smirk\",\n",
    "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":‑\\|\":\"Straight face\",\n",
    "    u\":\\|\":\"Straight face\",\n",
    "    u\":$\":\"Embarrassed or blushing\",\n",
    "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\"O:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑3\":\"Angel, saint or innocent\",\n",
    "    u\"0:3\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
    "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
    "    u\">:‑\\)\":\"Evil or devilish\",\n",
    "    u\">:\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:‑\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:\\)\":\"Evil or devilish\",\n",
    "    u\"3:‑\\)\":\"Evil or devilish\",\n",
    "    u\"3:\\)\":\"Evil or devilish\",\n",
    "    u\">;\\)\":\"Evil or devilish\",\n",
    "    u\"\\|;‑\\)\":\"Cool\",\n",
    "    u\"\\|‑O\":\"Bored\",\n",
    "    u\":‑J\":\"Tongue-in-cheek\",\n",
    "    u\"#‑\\)\":\"Party all night\",\n",
    "    u\"%‑\\)\":\"Drunk or confused\",\n",
    "    u\"%\\)\":\"Drunk or confused\",\n",
    "    u\":-###..\":\"Being sick\",\n",
    "    u\":###..\":\"Being sick\",\n",
    "    u\"<:‑\\|\":\"Dump\",\n",
    "    u\"\\(>_<\\)\":\"Troubled\",\n",
    "    u\"\\(>_<\\)>\":\"Troubled\",\n",
    "    u\"\\(';'\\)\":\"Baby\",\n",
    "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
    "    u\"\\(\\^_-\\)\":\"Wink\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
    "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
    "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
    "    u\"\\^_\\^\":\"Joyful\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
    "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
    "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;_;\":\"Sad of Crying\",\n",
    "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
    "    u\";_;\":\"Sad or Crying\",\n",
    "    u\";-;\":\"Sad or Crying\",\n",
    "    u\";n;\":\"Sad or Crying\",\n",
    "    u\";;\":\"Sad or Crying\",\n",
    "    u\"Q\\.Q\":\"Sad or Crying\",\n",
    "    u\"T\\.T\":\"Sad or Crying\",\n",
    "    u\"QQ\":\"Sad or Crying\",\n",
    "    u\"Q_Q\":\"Sad or Crying\",\n",
    "    u\"\\(-\\.-\\)\":\"Shame\",\n",
    "    u\"\\(-_-\\)\":\"Shame\",\n",
    "    u\"\\(一一\\)\":\"Shame\",\n",
    "    u\"\\(；一_一\\)\":\"Shame\",\n",
    "    u\"\\(=_=\\)\":\"Tired\",\n",
    "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
    "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
    "    u\"=_\\^=\t\":\"cat\",\n",
    "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
    "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
    "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
    "    u\"\\(\\・\\・?\":\"Confusion\",\n",
    "    u\"\\(?_?\\)\":\"Confusion\",\n",
    "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
    "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
    "    u\"\\^/\\^\":\"Normal Laugh\",\n",
    "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
    "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
    "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
    "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
    "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
    "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
    "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
    "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
    "    u'\\(-\"-\\)':\"Worried\",\n",
    "    u\"\\(ーー;\\)\":\"Worried\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
    "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
    "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
    "    u\":O o_O\":\"Surprised\",\n",
    "    u\"o_0\":\"Surprised\",\n",
    "    u\"o\\.O\":\"Surpised\",\n",
    "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
    "    u\"oO\":\"Surprised\",\n",
    "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
    "    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\n",
    "}\n",
    "def replace_emoticons(text):\n",
    "    for i in EMOTICONS:\n",
    "        text=re.sub(i,'_'.join(EMOTICONS[i].replace(',',' ').split()),text)\n",
    "    return text\n",
    "train_data['text']=train_data['text'].apply(lambda text:replace_emoticons(text))\n",
    "test_data['text']=test_data['text'].apply(lambda text:replace_emoticons(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#移除表情符號(emoji)\n",
    "def remove_emoji(text):\n",
    "    txt_emoji_list = emoji.distinct_emoji_list(text) #擷取emoji\n",
    "    rx='['+ re.escape(''.join(txt_emoji_list)) +']' #用 re.escape 將emoji轉成正規表達式，在用[]包裝成集合\n",
    "\n",
    "    if not txt_emoji_list:\n",
    "        return text\n",
    "    elif '©' not in txt_emoji_list:\n",
    "        return re.sub(rx,'',text)+\\\n",
    "        ''.join([emoji.demojize(emo).replace(':','') for emo in txt_emoji_list])# demojize() 會出現:_:的形式\n",
    "    elif '©' in txt_emoji_list:\n",
    "        if len(txt_emoji_list)==1:\n",
    "            return text.replace('©','')\n",
    "        elif len(txt_emoji_list)!=1:\n",
    "            text=text.replace('©','')\n",
    "            return re.sub(rx,'',text)+\\\n",
    "            ''.join([emoji.demojize(emo).replace(':','') for emo in txt_emoji_list])\n",
    "train_data['text']=train_data['text'].apply(lambda text:remove_emoji(text))#test_data 中無 emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74193f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#移除html tag\n",
    "def remove_html(text):\n",
    "    html=re.sub(r'<.*?>','',text)\n",
    "    text_no_whitespace=re.sub(r'\\s{2,}',' ',html)\n",
    "    return text_no_whitespace\n",
    "'''example = \"\"\"<div>\n",
    "<h1>Real or Fake</h1>\n",
    "<p>Kaggle </p>\n",
    "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
    "</div>\"\"\"\n",
    "print(remove_html(example))'''\n",
    "train_data['text']=train_data['text'].apply(lambda text:remove_html(text))\n",
    "test_data['text']=test_data['text'].apply(lambda text:remove_html(text))\n",
    "'''\n",
    "output = pd.DataFrame(train_data, columns=['text'])\n",
    "output.to_csv(r'C:\\downloded dataset\\twitter disater\\processed_data\\spell_corrected_text_tt.csv', index=False)'''\n",
    "output1=pd.DataFrame(test_data,columns=['text'])\n",
    "output1.to_csv(r'C:\\downloded dataset\\twitter disater\\processed_data\\test_havnt_spell_corrected_text_tt.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f83808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ems1 ny emts petition 17 per hour 繩minimum wage穠 httpskepticalannoyedundecideduneasyorhesitanttco4oa6swlxmr ems paramedics ambulance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"output = pd.DataFrame(train_data, columns=['text'])\\noutput.to_csv(r'C:\\\\downloded dataset\\twitter disater\\\\processed_data\\\\spell_corrected_text_3.csv', index=False)\\noutput1=pd.DataFrame(test_data,columns=['text'])\\noutput1.to_csv(r'C:\\\\downloded dataset\\twitter disater\\\\processed_data\\test_spell_corrected_text_3.csv',index=False)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#移除標點符號\n",
    "puts=string.punctuation #python中的標點符號\n",
    "def remove_punctuatuon(text):\n",
    "    return text.translate(str.maketrans('','',puts))\n",
    "ex='ems1 ny emts petition 17 per hour ?繩minimum wage?穠 httpskepticalannoyedundecideduneasyorhesitanttco4oa6swlxmr ems paramedics ambulance'\n",
    "print(remove_punctuatuon(ex))\n",
    "train_data['text']= train_data[\"text\"].apply(lambda text:remove_punctuatuon(text))\n",
    "test_data['text']=test_data['text'].apply(lambda text:remove_punctuatuon(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf0f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#移除連結\n",
    "def scrape_links(text):\n",
    "    url = re.compile(r'http\\w+')\n",
    "    return url.sub(r'',text)\n",
    "train_data['text']= train_data[\"text\"].apply(lambda text:scrape_links(text))\n",
    "test_data['text']=test_data['text'].apply(lambda text:scrape_links(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b26357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#移除特殊字元\n",
    "s_chars = '¥₽ÏïŰŬĎŸæ₿₪ÚŇÀèÅ”ĜåŽÖéříÿý€ŝĤ₹áŜŮÂ₴ûÌÇšŘúüëÓ₫ŠčÎŤÆÒœ₩öËäøÍťìĈôàĥÝ¢ç“žðÙÊĉŭÈŒÐÉÔĵùÁů„âÄűĴóêĝÞîØòď฿ČÜþňÛ'\n",
    "def remove_spec(text):\n",
    "    text = re.sub(r'[\\t\\s\\n\\r\\b\\a]', ' ', text)\n",
    "    text=re.sub(r'[{}]'.format(s_chars),'',text)\n",
    "    text = re.sub(r'(\\s[^iIaA]\\s)', ' ', text)\n",
    "    return text\n",
    "train_data['text']= train_data[\"text\"].apply(lambda text:remove_spec(text))\n",
    "test_data['text']=test_data['text'].apply(lambda text:remove_spec(text))\n",
    "example03 = train_data.iloc[2171,0]\n",
    "#print(remove_spec(example03))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffd779c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#統一大小寫\n",
    "train_data['text']=train_data['text'].str.lower()\n",
    "test_data['text']=test_data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2443d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#拼寫糾正\n",
    "from tqdm import tnrange, tqdm_notebook,tqdm\n",
    "from time import sleep\n",
    "\n",
    "def correct_wd(text):\n",
    "    correct_text=[]\n",
    "    f=0\n",
    "    zzz=tqdm_notebook(text.split())\n",
    "    for i in zzz:\n",
    "        f+=1\n",
    "        i=str(i)\n",
    "        i=Word(i)\n",
    "        c=i.spellcheck()\n",
    "        zzz.set_description('{}th iteration'.format(f))\n",
    "        correct_text.append(c[0][0])\n",
    "    return ' '.join(correct_text)\n",
    "train_data['text']=train_data['text'].apply(lambda text:correct_wd(text))\n",
    "test_data['text']=test_data['text'].apply(lambda text:correct_wd(text))\n",
    "\n",
    "#context = 'Strawberyy is a beauity'\n",
    "#print(correct_wd(context))\n",
    "output = pd.DataFrame(train_data, columns=['text'])\n",
    "output.to_csv(r'C:\\downloded dataset\\twitter disater\\processed_data\\spell_corrected_text_tt.csv', index=False)\n",
    "output1=pd.DataFrame(test_data,columns=['text'])\n",
    "output1.to_csv(r'C:\\downloded dataset\\twitter disater\\processed_data\\test_spell_corrected_text_tt.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bcce33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#處理數字\n",
    "train_data['text']=train_data['text'].str.replace('\\d+','',regex=True)\n",
    "test_data['text']=test_data['text'].str.replace('\\d+','',regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25cb9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=re.compile(r'(.)\\1{3,}')\n",
    "def reduce_length(text):\n",
    "    return pattern.sub(r'\\1',text)\n",
    "train_data['text']=train_data['text'].apply(lambda text:reduce_length(text))\n",
    "test_data['text']=test_data['text'].apply(lambda text:reduce_length(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fba4603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         deeds reason earthquake may allah forgive us\n",
      "1                 forest fire near la range ask canada\n",
      "2    residents asked shelter place notified officer...\n",
      "3    people receive wildfires evacuation orders cal...\n",
      "4    got sent photo ruby alaska smoke wildfires hou...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 去除停用詞\n",
    "stopwords_list=stopwords.words('english')\n",
    "print(stopwords_list)\n",
    "def remove_stopwords(text):\n",
    "    correct_words=[]\n",
    "    for i in text.split():\n",
    "        if i in stopwords_list:\n",
    "            continue\n",
    "        else:\n",
    "            correct_words.append(i)\n",
    "    return ' '.join(correct_words)\n",
    "train_data['text']=train_data['text'].apply(lambda text:remove_stopwords(text))\n",
    "test_data['text']=test_data['text'].apply(lambda text:remove_stopwords(text))\n",
    "print(train_data['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ee31eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'amp', 'fire', 'get', 'via', 'new', 'news', 'dont', 'people', 'us', 'symptoms', 'developing', 'bathandnortheastsomerset', 'overtaking', 'fatherofthree', 'explosivespacked', 'explodingkittens\\x89', 'gameofkittens', 'exploding']\n"
     ]
    }
   ],
   "source": [
    "#去掉高低頻詞\n",
    "DISASTER_WORDS = ['fire', 'kill', 'bomb', 'disaster', 'crash', 'flood',\n",
    "                  'suicide', 'police', 'attack', 'storm', 'emergency', 'burn']\n",
    "\n",
    "NON_DISASTER_WORDS = ['like', 'new', 'body', 'love', 'good']\n",
    "cnt=Counter()\n",
    "for text in train_data[\"text\"]:\n",
    "    for word in text.split():\n",
    "        cnt[word]+=1\n",
    "most=cnt.most_common(10)\n",
    "least=cnt.most_common()[:-10:-1]\n",
    "\n",
    "mot_and_lets=[]\n",
    "for i in most:\n",
    "    mot_and_lets.append(i[0])\n",
    "for i in least:\n",
    "    mot_and_lets.append(i[0])\n",
    "print(mot_and_lets)\n",
    "for word in DISASTER_WORDS + NON_DISASTER_WORDS:\n",
    "    if word in mot_and_lets:\n",
    "        mot_and_lets.pop(mot_and_lets.index(word))\n",
    "def remove_freq(text):\n",
    "        return \" \".join([word for word in text.split() if word not in mot_and_lets])\n",
    "train_data['text']=train_data['text'].apply(lambda text:remove_freq(text))\n",
    "test_data['text']=test_data['text'].apply(lambda text:remove_freq(text))\n",
    "output = pd.DataFrame(train_data, columns=['text'])\n",
    "output.to_csv(r'C:\\downloded dataset\\twitter disater\\processed_data\\spell_corrected_text_tt_final.csv', index=False)\n",
    "output1=pd.DataFrame(test_data,columns=['text'])\n",
    "output1.to_csv(r'C:\\downloded dataset\\twitter disater\\processed_data\\test_spell_corrected_text_tt_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e46d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
